{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math of Backpropagation\n",
    "\n",
    "## The math has always been intimidating to all of us.In this article I'll try to derive some of the scary looking math,while trying to make sense out of most of it. :)\n",
    "\n",
    "### Prerequisites:\n",
    "- A little knowledge about Calculus(chain rules & partial derivatives),I'll try to cite some of the resources of the same which might come handy,in case you have forgotten it.\n",
    "- Familiarization with Neural Networks.\n",
    "- A little bit of patience will go a long way in this journey. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's define some of the variables that we'll be seeing along the way:\n",
    "\n",
    "### Notational nuances:\n",
    "- **Superscript defines layer number**\n",
    "- **Subscript defines feature number or activation unit number**\n",
    "\n",
    "### Variables and their meaning:\n",
    "- $\\vec{x}$= $\\vec{a^{0}}$=vector of input features= $ \\begin{bmatrix}x_{1} \\\\ x_{2} \\\\ .\\\\.\\\\.\\end {bmatrix} $\n",
    "- $ W^{i}_{c \\times d} $ = Weight matrix from each node of layer i to each node of layer (i+1).\n",
    "    - **c** is the no. of nodes in the (i+1)th layer.\n",
    "    - **d** is no. of nodes of i th layer\n",
    "- $ \\vec{z^{i}} $= $ W^{i-1} \\dot{\\vec{a^{i-1}}}+\\vec{b}^{i-1} $ = Linear transformation\n",
    "- $\\vec{a^{i}}$=vector of activation units of ith layer= $ \\begin{bmatrix}a^{i}_{1} \\\\ a^{i}_{2} \\\\.\\\\.\\\\.\\end {bmatrix} $\n",
    "- $ \\sigma(z) $ =$\\frac{1}{1+e^{-z}}$ [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function applied on {z}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some visual intuition of how backprop updation works in a shallow NN ###\n",
    "<img src=\"backprop.gif\" style=\"width:400;height:400;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll be working with a simple model like:\n",
    "<img src=\"NN.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "**Mathematically**\n",
    "\n",
    "$z^{1}_{j}=W^{[1](j)}x + b; where\\ j\\in\\{1,2,3,4\\} $\n",
    "\n",
    "$a^{1}_{j}= act(z^{1}_{j}); where\\ j\\in\\{1,2,3,4\\}$ (act() is any activation function)\n",
    "\n",
    "**We can stack all the values of a in form of vectors as $ \\vec{a^{i}}$ **\n",
    "\n",
    "$z^{(2)} = W^{[2]}a^{T} + b $\n",
    "$a^{(2)} = \\sigma(z^{2})$\n",
    "\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "$ L(a^{(2)},y)= -y\\log(a^{(2)}) + (1-y)\\log(1- a^{(2)})$\n",
    "\n",
    "$J=\\frac{1}{m}\\sum_{1}^{m}L(a^{(2)},y)$\n",
    "<img src=\"gradients.PNG\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "$\\frac{\\partial J}{\\partial a^{(2)}}= \\frac{1}{m}(-\\frac{y}{a^{(2)}} + \\frac{1-y}{1-a^{(2)}})......(1) $\n",
    "\n",
    "$\\frac{\\partial a^{(2)}}{\\partial z^{(2)}} = a^{(2)}(1-a^{(2)})$ ($\\because a=\\sigma(z^{(2)})......(2)$ and $\\sigma(z^{(2)})^{'}=\\sigma(z^{(2)})(1-\\sigma(z^{(2)})$\n",
    "\n",
    "$\\frac{\\partial z^{(2)}}{\\partial w^{(2)}}= a^{(1)} (\\because z^{(2)}=w^{(2)}a^{(1)} +b^{(2)})......(3)$\n",
    "\n",
    "\n",
    "$\\frac{\\partial z^{(2)}}{\\partial b^{(2)}}= 1 (\\because z^{(2)}=w^{(2)}a^{(1)} +b^{(2)})......(4)$\n",
    "\n",
    "\n",
    "**Now,to find $\\frac{\\partial J}{\\partial w^{(2)}}$ we have to use chain-rule of calculus as**\n",
    "$\\frac{\\partial J}{\\partial w^{(2)}}$=$\\frac{\\partial J}{\\partial a^{(2)}}\\times \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\times \\frac{\\partial z^{(2)}}{\\partial w^{(1)}} $=$(a^{(2)}-y)*a^{(1)}$(By combining (1),(2) &(3))\n",
    "\n",
    "**Similarly we can find $\\frac{\\partial J}{\\partial b_{i}}$ as**\n",
    "$\\frac{\\partial J}{\\partial b_{2}}$=$\\frac{\\partial J}{\\partial a^{(2)}}\\times \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\times \\frac{\\partial z^{(2)}}{\\partial b^{(2)}} $=$(a^{(2)}-y)$(By combining (1),(2) &(4))\n",
    "\n",
    "**To update paramters we use gradient descent updation rule [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)**\n",
    "- $w^{(2)}$=$w^{(2)}- \\alpha \\frac{\\partial J}{\\partial w^{(2)}}$\n",
    "- $b^{(2)}$=$b^{(2)}- \\alpha \\frac{\\partial J}{\\partial b^{(2)}}$\n",
    "\n",
    "#### Backpropagation goes in the reverse direction and updates all the weights using the above updation rule.A good analogy of backpropagation can be called as dynamic programming(Where we use pre-computed values).Similar to DP,here we store the values of derivates and use it in the chain rule during the backward pass.This notion can be easily understood with the computation graph shown below,where the edge direction indicates dependence.  ####\n",
    "\n",
    "<img src=\"dependency.jpg\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have just given a very basic high level overview of the derivation.The actual derivation remains as one of the most challenging aspects of NN.But if you have understood the intuition behind it,you're almost through this most challenging topic,Voilaa!! :)##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
