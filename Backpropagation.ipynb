{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math of Backpropagation\n",
    "\n",
    "## The math has always been intimidating to all of us.In this article I'll try to derive some of the scary looking math,while trying to make sense out of most of it. :)\n",
    "\n",
    "### Prerequisites:\n",
    "- A little knowledge about Calculus(chain rules & partial derivatives),I'll try to cite some of the resources of the same which might come handy,in case you have forgotten it.\n",
    "- Familiarization with Neural Networks.\n",
    "- A little bit of patience will go a long way in this journey. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's define some of the variables that we'll be seeing along the way:\n",
    "\n",
    "### Notational nuances:\n",
    "- **Superscript defines layer number**\n",
    "- **Subscript defines feature number or activation unit number**\n",
    "\n",
    "### Variables and their meaning:\n",
    "- $\\vec{x}$= $\\vec{a^{0}}$=vector of input features= $ \\begin{bmatrix}x_{1} \\\\ x_{2} \\\\ .\\\\.\\\\.\\end {bmatrix} $\n",
    "- $ W^{i}_{c \\times d} $ = Weight matrix from each node of layer i to each node of layer (i+1).\n",
    "    - **c** is the no. of nodes in the (i+1)th layer.\n",
    "    - **d** is no. of nodes of i th layer\n",
    "- $ \\vec{z^{i}} $= $ W^{i-1} \\dot{\\vec{a^{i-1}}}+\\vec{b}^{i-1} $ = Linear transformation\n",
    "- $\\vec{a^{i}}$=vector of activation units of ith layer= $ \\begin{bmatrix}a^{i}_{1} \\\\ a^{i}_{2} \\\\.\\\\.\\\\.\\end {bmatrix} $\n",
    "- $ \\sigma(z) $ = [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function applied on {z}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some visual intuition of how backprop updation works in a shallow NN ###\n",
    "<img src=\"backprop.gif\" style=\"width:400;height:400;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll be working with a simple model like:\n",
    "<img src=\"NN.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "**Mathematically**\n",
    "\n",
    "$$z^{1}_{j}=W^{[1](j)}x + b; where\\ j\\in\\{1,2,3,4\\} $$\n",
    "\n",
    "$$a^{1}_{j}= act(z^{1}_{j}); where\\ j\\in\\{1,2,3,4\\}$$ (act() is any activation function)\n",
    "\n",
    "**We can stack all the values of a in form of vectors as $ \\vec{a^{i}}$ **\n",
    "\n",
    "$$z^{2} = W^{[2]}a^{T} + b $$\n",
    "$$a^{2} = \\sigma(z^{2})$$\n",
    "\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "Given the predictions on all the examples, you can also compute the cost $J$ as follows: \n",
    "$ J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m}-y^{(i)}\\log(a^{[2](i)}) + (1-y^{(i)})\\log(1-a^{[2](i)})$\n",
    "\n",
    "**Now,to find $\\frac{\\partial J}{\\partial w_{i}}$ we have to use chain-rule of calculus as**\n",
    "$\\frac{\\partial J}{\\partial w_{i}}$=$\\frac{\\partial J}{\\partial a}\\times \\frac{\\partial a}{\\partial z} \\times \\frac{\\partial z}{\\partial w_{i}} $=$(a-y)*w_{i}$\n",
    "\n",
    "**Similarly we can find $\\frac{\\partial J}{\\partial b_{i}}$ as**\n",
    "$\\frac{\\partial J}{\\partial b_{i}}$=$\\frac{\\partial J}{\\partial a}\\times \\frac{\\partial a}{\\partial z} \\times \\frac{\\partial z}{\\partial w_{i}} $=$(a-y)$\n",
    "\n",
    "**To update paramters we use gradient descent updation rule [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)**\n",
    "- $w_{i}$=$w_{i}- \\alpha \\frac{\\partial J}{\\partial w_{i}}$\n",
    "- $b_{i}$=$b_{i}- \\alpha \\frac{\\partial J}{\\partial b_{i}}$\n",
    "\n",
    "#### Backpropagation goes in the reverse direction and updates all the weights using the above updation rule.A good analogy of backpropagation can be called as dynamic programming(Where we use pre-computed values).Similar to DP,here we store the values of derivates and use it in the chain rule during the backward pass.This notion can be easily understood with the computation graph shown below,where the edge direction indicates dependence.  ####\n",
    "\n",
    "<img src=\"dependency.jpg\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have just given a very basic high level overview of the derivation.The actual derivation remains as one of the most challenging aspects of NN.But if you have understood the intuition behind it,you're almost through this most challenging topic,Voilaa!! :)##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
